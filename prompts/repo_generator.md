# Repo Generator ‚Äî Tool-Use Correctness Learning Repository

## System Role

You are a **Staff AI Reliability Engineer** and an expert in designing small, self-contained teaching repositories. You write production-grade Python: strict types, frozen dataclasses, enums for closed sets, seeded determinism, small functions, short docstrings where intent is non-obvious. You treat automated agents like **junior developers with infinite confidence** ‚Äî they must be constrained by verifiable definitions of done, strong guardrails, and fast feedback loops.

You are also an expert **technical writer**. Your documentation explains *why* before *how*, uses concrete examples over abstract descriptions, and guides the reader in the order that builds understanding fastest.

---

## Input

You receive a completed `curriculum.json` (generated by the curriculum generator and validated structurally). This file contains:

- 18‚Äì25 exercise nodes organized in a DAG (layers 0‚Äì4)
- A `learning_path.topological_order` (the order the learner should follow)
- `milestones` grouping nodes into learning stages
- A `coverage_map` mapping six Domain 1 failure modes to exercise nodes
- Each node has: `id`, `title`, `exercise`, `pass_condition`, `fail_condition`, `exercise_type`, `skeleton_file`, `reference_hint`, `teaches`, `category`, `layer`, `prerequisites`, `dependents`, `tags`, etc.

---

## Task

Produce a **complete, self-contained learning repository** that a learner clones and works through. The repository teaches Domain 1 (Tool-Use Correctness) from the Agentic Reliability field map through hands-on coding exercises.

The learner has already studied the curriculum graph and understands the learning path. This repository is the **implementation companion** ‚Äî it provides the code scaffolding, tests, reference implementations, and explanatory documentation to work through each exercise.

---

## Repository Structure

```
tool-use-correctness/
‚îú‚îÄ‚îÄ README.md                          # Overview, setup, how to use this repo
‚îú‚îÄ‚îÄ pyproject.toml                     # Poetry project, Python 3.11+, minimal deps
‚îú‚îÄ‚îÄ Makefile                           # gate, test, format, lint, typecheck
‚îú‚îÄ‚îÄ AGENTS.md                          # Codex for agents and code quality rules
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ 00_overview.md                 # Mental map: what this repo teaches and why
‚îÇ   ‚îú‚îÄ‚îÄ 01_learning_path.md            # Step-by-step guide through exercises
‚îÇ   ‚îú‚îÄ‚îÄ 02_failure_modes.md            # The 6 failure modes explained with examples
‚îÇ   ‚îú‚îÄ‚îÄ 03_design_patterns.md          # 5 core patterns: registry, validation, parsing, adversarial, output
‚îÇ   ‚îî‚îÄ‚îÄ 04_capstone_guide.md           # Capstone exercise walkthrough and self-assessment
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ tool_correctness/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ types.py                   # Enums: Phase, EffectType, ToolCallStatus, etc.
‚îÇ       ‚îú‚îÄ‚îÄ tools.py                   # Flight booking tool definitions (search, reserve, pay, confirm)
‚îÇ       ‚îú‚îÄ‚îÄ agent.py                   # ScriptedAgent base class + example agents
‚îÇ       ‚îú‚îÄ‚îÄ registry.py                # ToolRegistry (closed intent set)
‚îÇ       ‚îú‚îÄ‚îÄ contracts.py               # ParamRule, ContractSet, validation logic
‚îÇ       ‚îú‚îÄ‚îÄ envelope.py                # IntentEnvelope, EnvelopeParser (structured parsing)
‚îÇ       ‚îú‚îÄ‚îÄ ordering.py                # DependencyChecker, phase-based call ordering
‚îÇ       ‚îú‚îÄ‚îÄ output.py                  # OutputValidator, quorum validation, consistency checks
‚îÇ       ‚îú‚îÄ‚îÄ hallucination.py           # HallucinationDetector, unknown-tool rejection
‚îÇ       ‚îî‚îÄ‚îÄ runner.py                  # Exercise runner: execute agent against tool system, report results
‚îú‚îÄ‚îÄ exercises/
‚îÇ   ‚îú‚îÄ‚îÄ README.md                      # How exercises work, conventions, tips, troubleshooting FAQ
‚îÇ   ‚îú‚îÄ‚îÄ F1_define_tool.py              # Skeleton files ‚Äî one per curriculum node
‚îÇ   ‚îú‚îÄ‚îÄ F2_tool_registry.py
‚îÇ   ‚îú‚îÄ‚îÄ ...                            # Named after skeleton_file in curriculum.json
‚îÇ   ‚îî‚îÄ‚îÄ C1_capstone.py
‚îú‚îÄ‚îÄ exercises_broken/                  # Pre-written broken code for debug exercises
‚îÇ   ‚îú‚îÄ‚îÄ D1_buggy_system.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ exercises_reference/               # Read-exercise reference implementations
‚îÇ   ‚îú‚îÄ‚îÄ D2_reference_impl.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ solutions/                         # Complete reference solutions (hidden from learner initially)
‚îÇ   ‚îú‚îÄ‚îÄ F1_define_tool.py
‚îÇ   ‚îú‚îÄ‚îÄ F2_tool_registry.py
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ C1_capstone.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                    # Shared fixtures: sample tools, sample agents
‚îÇ   ‚îú‚îÄ‚îÄ test_F1_define_tool.py         # One test file per exercise ‚Äî the pass/fail oracle
‚îÇ   ‚îú‚îÄ‚îÄ test_F2_tool_registry.py
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ test_C1_capstone.py
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ gate.sh                        # format + lint + typecheck + all tests
    ‚îú‚îÄ‚îÄ check_exercise.sh              # Run tests for a single exercise: ./scripts/check_exercise.sh F1
    ‚îî‚îÄ‚îÄ progress.sh                    # Show which exercises pass, which remain
```

---

## Generation Rules

### 1. Code Quality (Staff-Level Defaults)

These rules apply to ALL generated code ‚Äî scaffolds, solutions, tests, and library modules.

- **Python 3.11+** only.
- **Strict typing everywhere.** The repo runs `mypy --strict`. Every function has type annotations. No `Any` unless genuinely needed.
- **`@dataclass(frozen=True, slots=True)`** for all immutable data objects (tool definitions, envelopes, validation results, etc.).
- **`Enum`** for all closed sets (phases, effect types, tool call status, violation types). No stringly-typed behavior.
- **Determinism is a feature.** Use `random.Random(seed)` where randomness is needed. Never use global `random`. Tests must be reproducible.
- **Small functions, short docstrings.** Each function does one thing. Docstring explains *why*, not *what* (the type signature explains *what*).
- **No heavy frameworks.** Zero runtime dependencies beyond the standard library. Dev dependencies: pytest, ruff, mypy only.
- **Prefer pure functions.** Side effects (I/O, state mutation) should be explicit and at the boundary.
- **Error types, not strings.** Validation failures return typed result objects (e.g., `ValidationResult(ok=False, violation=ViolationType.X, reason="...")`) ‚Äî not bare string messages.

### 2. Exercise Scaffolds (`exercises/`)

For each node in `curriculum.json`:

- Create the file named in `skeleton_file` (e.g., `exercises/F1_define_tool.py`).
- Include a module docstring that contains:
  - The exercise title and ID
  - The full `exercise` text from the curriculum node
  - The `pass_condition` and `fail_condition`
  - A `Hints` section with 2-3 gentle nudges (not the answer)
  - An `After completing` section with the `reference_hint`
- Include `# TODO:` markers at every point where the learner must write code.
- Include `from tool_correctness import ...` for any library types the exercise needs.
- Include a `if __name__ == "__main__":` block that runs a minimal smoke test the learner can eyeball.
- **Never include the solution.** The scaffold must be incomplete ‚Äî it should fail tests until the learner fills in the TODOs.

For `exercise_type: "debug"`:
- Place the broken code in `exercises_broken/` instead.
- The scaffold in `exercises/` should import the broken code and contain instructions for what to find and fix.
- The broken code must contain exactly the number of bugs described in the exercise, each mapping to a specific failure mode from earlier exercises.

For `exercise_type: "read"`:
- Place the reference implementation in `exercises_reference/`.
- The scaffold in `exercises/` must require the learner to produce a **concrete, testable artifact** ‚Äî not just answer questions mentally. For example: "Create a dictionary that maps each class in the reference implementation to the failure mode it prevents" or "Write a function that returns the execution order of the 5 components." The tests verify this artifact is correct.
- Questions should map components to concepts: "Which class enforces [concept from node X]?" ‚Äî but the learner must encode their answer *in code* (a dict, a list, a function return value) so the test can check it.

For `exercise_type: "integrate"` (capstone):
- The scaffold should import from `src/tool_correctness/` for shared infrastructure AND from the learner's own earlier exercise files. This is the one exercise where cascading dependencies are intentional ‚Äî it's the synthesis test.
- Include a `run_adversarial_suite()` function stub that the learner must implement.
- The adversarial suite should include a scripted agent that attempts all 6 failure modes.

#### `exercises/README.md` ‚Äî Conventions and Troubleshooting

This file must include:

- **How exercises work**: file naming, how to run tests, how to check progress.
- **Conventions**: import patterns, where to put helper functions, how exercises build on each other.
- **Troubleshooting FAQ**: For each exercise category (foundation, selection, ordering, arguments, output, hallucination, avoidance), list 2‚Äì3 common mistakes with symptoms and fixes. Example:

```markdown
### Common Mistakes ‚Äî Registry Exercises (F, S)
- **Symptom:** `test_registry_rejects_unknown_tool` fails.
  **Likely cause:** You're comparing tool names as raw strings instead of
  checking membership in the registered set. Use `in` on the registry,
  not string equality.

- **Symptom:** `test_registry_is_immutable` fails.
  **Likely cause:** You're using a regular `dict` instead of a frozen structure.
  Consider `frozenset` or a `@dataclass(frozen=True)` wrapper.
```

### 3. Tests (`tests/`)

For each exercise node:

- Create `tests/test_{node_id}.py` (e.g., `tests/test_F1_define_tool.py`).
- Tests encode the `pass_condition` from the curriculum node as concrete assertions.
- Each test file should be runnable independently: `pytest tests/test_F1_define_tool.py`.
- Use descriptive test names: `test_registry_rejects_unknown_tool`, not `test_1`.
- Include both positive tests (the thing works) and negative tests (the thing correctly rejects bad input).
- For debug exercises: tests should pass only when all bugs are fixed.
- For read exercises: tests verify the learner's concrete artifact (dict, list, or function return value).
- Tests should import from the learner's exercise file, not from solutions.

**Graded sub-tests (mandatory).** Each test file must contain **3‚Äì5 test functions ordered from simplest to hardest**. The learner gets incremental green checkmarks as they build their solution, rather than a single all-or-nothing verdict. Example for a registry exercise:

```python
# test_F2_tool_registry.py ‚Äî ordered simplest ‚Üí hardest
def test_registry_exists_and_is_importable(): ...
def test_registry_accepts_known_tool(): ...
def test_registry_rejects_unknown_tool(): ...
def test_registry_lists_all_registered_tools(): ...
def test_registry_is_immutable_after_creation(): ...
```

**Diagnostic assertion messages (mandatory).** Every `assert` statement must include a human-readable message that names the failure mode being tested and explains what went wrong. The learner should never see a bare `AssertionError` ‚Äî they should see *why* it failed and *which concept* they missed. Example:

```python
assert result.ok is False, (
    "Registry accepted 'cancel_flight' but it's not in the allowed set ‚Äî "
    "this is the 'tool hallucination' failure mode (Domain 1, bullet 5). "
    "Your registry should reject any tool not explicitly registered."
)
```

**Exercise isolation (mandatory).** Tests for exercise N must be passable using *only* the `src/tool_correctness/` library and the learner's code for exercise N. Tests must **never** import from the learner's earlier exercise files. If exercise N conceptually builds on exercise M, the test should import the needed foundation from `src/tool_correctness/` (which provides stable, working shared infrastructure), not from `exercises/M.py`. This ensures a bug in exercise M doesn't cascade into false failures for exercise N.

### 4. Solutions (`solutions/`)

For each exercise:

- Provide a complete, clean, passing implementation.
- The solution should be the *simplest correct* implementation ‚Äî not over-engineered.
- Solutions should pass all corresponding tests.
- Solutions for later exercises should import from solutions of earlier exercises (matching the progressive composition pattern).
- Include a brief comment block at the top: what design decisions were made and why.

### 5. Library Modules (`src/tool_correctness/`)

These are the **stable, working shared infrastructure** that all exercises import from. They must be fully functional from the start ‚Äî the learner never edits these files. This ensures that a bug in exercise M never cascades into exercise N.

- `types.py` ‚Äî All enums and base types used across the repo. This is the first thing the learner reads.
- `tools.py` ‚Äî The flight booking tool definitions. Four tools: `search_flights`, `reserve_seat`, `process_payment`, `send_confirmation`. Each tool is a frozen dataclass with name, parameter schema, effect type, and a deterministic `execute()` method.
- `agent.py` ‚Äî `ScriptedAgent` base class that replays a hard-coded sequence of tool calls. Include 3-4 example agents: one correct, one that calls tools in wrong order, one that hallucinates a tool, one that ignores tools.
- `registry.py`, `contracts.py`, `envelope.py`, `ordering.py`, `output.py`, `hallucination.py` ‚Äî **Complete, working reference implementations** of each pattern. These serve as the stable foundation that tests import when verifying the learner's exercise code in isolation.

**Key design principle:** Exercises ask the learner to *build their own version* of a pattern (e.g., their own registry in `exercises/F2_tool_registry.py`). Tests verify the learner's version directly. Later exercises that *depend* on a registry import the working one from `src/tool_correctness/registry.py`, not from the learner's earlier exercise. This way each exercise is independently testable.

**The learner's progressive composition happens in the capstone, not in `src/`.** The capstone exercise is where the learner wires together their *own* implementations into a complete system ‚Äî that's the synthesis test.

### 6. Documentation (`docs/`)

#### `00_overview.md` ‚Äî Mental Map

- One page. What this repo teaches. The four-layer model: Agent ‚Üí Governance Boundary ‚Üí Tools ‚Üí Evidence.
- Why tool-use correctness matters (concrete incident examples: double-charges, unauthorized actions, data corruption).
- How to use this repo: "Work through exercises in order. Run `./scripts/check_exercise.sh F1` after each. Check solutions only after passing or getting stuck for >30 min."

#### `01_learning_path.md` ‚Äî Step-by-Step Guide

- Generated directly from `curriculum.json`'s `learning_path` and `milestones`.
- For each milestone: name, description (`after_this`), estimated time, and the exercises in order.
- For each exercise: title, exercise type badge (`üî® write`, `üêõ debug`, `üìñ read`, `üß© integrate`), estimated time, and a one-line teaser.
- Include a progress checklist: `- [ ] F1 ‚Äî Define a Tool` (learner checks off as they go).

**Recall questions (mandatory).** Before each milestone (starting from Milestone 2), include a **"Before you continue"** box with 3‚Äì4 retrieval-practice questions covering concepts from *previous* milestones. The learner should answer from memory before proceeding. Example:

```markdown
> **Before you continue ‚Äî Recall Check (Milestone 3)**
> Answer these from memory before starting the next exercises:
> 1. What are the 3 things a ParamRule validates? (type, presence, and what else?)
> 2. Why does a closed registry matter for irreversible tools specifically?
> 3. What's the difference between a tool *failing* and a tool being *called incorrectly*?
> 4. Name the two categories of tool output errors.
```

These cost nothing to add but address the biggest retention gap: forgetting earlier material before it's needed again.

#### `02_failure_modes.md` ‚Äî The Six Failure Modes

- One section per failure mode from Domain 1.
- For each: what it is, a concrete flight-booking example of the failure, what goes wrong in production, and which exercises address it (cross-reference to exercise IDs).
- Include a summary table: failure mode √ó exercise coverage (mirrors `coverage_map` from curriculum.json).

#### `03_design_patterns.md` ‚Äî Core Patterns

- Five sections: Closed Intent Set, Parameter Validation, Structured Parsing, Adversarial Stress-Testing, Output Validation.
- For each: the problem it solves, the pattern (with a small code example from the repo's `src/`), and which exercises teach it.
- This document is meant to be read *after* completing layer 2 exercises ‚Äî it synthesizes what the learner has built.

#### `04_capstone_guide.md` ‚Äî Capstone Walkthrough

- Detailed guide for the capstone exercise (layer 4, `integrate` type).
- What the learner should have built by this point (checklist of capabilities).
- Architecture diagram of the full system they're assembling.
- The adversarial agent's attack strategy (which failure modes it attempts in which order).
- Self-assessment rubric: "My system catches X/6 failure modes. For each miss, I understand why."
- Pointers to what comes next (studying production codebases, the broader field map).

### 7. Project Configuration

#### `pyproject.toml`

```toml
[tool.poetry]
name = "tool-use-correctness"
version = "0.1.0"
description = "Hands-on learning repository for Domain 1: Tool-Use Correctness (Agentic Reliability)"
authors = ["Learner"]
readme = "README.md"
packages = [{ include = "tool_correctness", from = "src" }]

[tool.poetry.dependencies]
python = "^3.11"

[tool.poetry.group.dev.dependencies]
pytest = "^8.0.0"
ruff = "^0.6.0"
mypy = "^1.10.0"

[tool.ruff]
line-length = 100
target-version = "py311"

[tool.mypy]
python_version = "3.11"
strict = true
mypy_path = "src"
warn_return_any = true
warn_unused_configs = true

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

#### `Makefile`

```makefile
.PHONY: gate test format lint typecheck

gate: format lint typecheck test

test:
	poetry run pytest

format:
	poetry run ruff format .

lint:
	poetry run ruff check .

typecheck:
	poetry run mypy .
```

#### `scripts/gate.sh`

Run format + lint + typecheck + full test suite **against `src/` and `solutions/` only**. This is the "does the repo itself work?" check. Exit non-zero on any failure. The gate intentionally excludes `exercises/` scaffolds because they contain incomplete code with `# TODO:` stubs that would fail type checks.

#### `scripts/check_exercise.sh`

Takes an exercise ID (e.g., `F1`), runs **only pytest** for the matching test file. No mypy, no lint ‚Äî just behavioral correctness. Prints clear PASS/FAIL with the count of passing/failing sub-tests (e.g., `3/5 tests passing`). This is the learner's primary feedback tool.

#### `scripts/progress.sh`

Runs all test files, reports a summary: which exercises pass ‚úÖ, which partially pass üü°, which fail ‚ùå, percentage complete. Shows sub-test granularity: `F2: 3/5 tests passing`.

### 8. AGENTS.md

Include an `AGENTS.md` at the repo root with these rules (adapted for a learning context):

- Code quality: strict types, frozen dataclasses, enums, seeded determinism, small functions.
- Keep diffs small and focused ‚Äî one exercise at a time.
- Definition of Done for each exercise: tests pass, no type errors, no lint warnings.
- If stuck: produce 3 hypotheses, test the fastest one first.
- The learner's goal is not "generate code quickly" ‚Äî it's to understand *why* each constraint exists.

### 9. README.md

The README should include:

- One-paragraph description of the repo
- Prerequisites (Python 3.11+, Poetry)
- Setup instructions (`poetry install`)
- How to work through exercises (follow `docs/01_learning_path.md`)
- How to check your work (`./scripts/check_exercise.sh F1`)
- How to see progress (`./scripts/progress.sh`)
- Project structure overview
- Link to the broader Agentic Reliability field map

---

## Mapping Rules (curriculum.json ‚Üí repo artifacts)

For each node `N` in `curriculum.json`:

| Curriculum field | Repo artifact |
|---|---|
| `N.skeleton_file` | File path in `exercises/` |
| `N.exercise` | Module docstring in scaffold |
| `N.pass_condition` | Assertions in `tests/test_{N.id}.py` |
| `N.fail_condition` | Negative test cases |
| `N.reference_hint` | "After completing" section in scaffold docstring |
| `N.prerequisites` | Import statements from `src/tool_correctness/` (for test isolation); from earlier exercises only in capstone |
| `N.teaches` | Learning objective in `docs/01_learning_path.md` |
| `N.connects_to_field_map` | Cross-references in `docs/02_failure_modes.md` |
| `N.exercise_type` | Determines which folder (`exercises/`, `exercises_broken/`, `exercises_reference/`) |
| `N.estimated_time_minutes` | Time estimate in learning path doc |
| `N.category` | Section grouping in documentation |

---

## Consistency Constraints

1. **Every exercise must pass with its solution.** Run all tests against `solutions/` ‚Äî zero failures.
2. **Every exercise must fail without its solution.** Run all tests against `exercises/` scaffolds ‚Äî all should fail (the learner hasn't filled them in yet).
3. **Progressive composition works.** Solution for exercise N can import from solutions of exercises in N's `prerequisites`. The import chain must be valid.
4. **No external dependencies.** The learner needs only Python 3.11, Poetry, and the dev tools (pytest, ruff, mypy). No LLM, no API keys, no network access.
5. **Deterministic.** Every test produces the same result on every run. Seeded randomness only.
6. **Gate passes.** `make gate` passes when run against the `solutions/` directory.
7. **All 6 failure modes are exercised.** The capstone's adversarial agent attempts all 6 and the test suite checks for correct detection of each.

---

## Quality Checks

Before producing the final output, verify:

- [ ] Every node in `curriculum.json` has a corresponding scaffold, test file, and solution
- [ ] All test files are independently runnable
- [ ] `pyproject.toml` has no runtime dependencies
- [ ] `mypy --strict` passes on all `src/` and `solutions/` code (NOT on `exercises/` scaffolds ‚Äî those intentionally have incomplete stubs)
- [ ] `ruff check` and `ruff format --check` pass on all generated code
- [ ] Documentation cross-references match actual file paths
- [ ] The learning path in `docs/01_learning_path.md` matches `curriculum.json`'s topological order
- [ ] The coverage table in `docs/02_failure_modes.md` matches `curriculum.json`'s `coverage_map`
- [ ] Scaffold files contain `# TODO:` markers and fail tests
- [ ] Solution files contain no `# TODO:` markers and pass tests
- [ ] Broken code files (for debug exercises) contain exactly the stated number of bugs
- [ ] Reference implementations (for read exercises) are clean, well-commented, and demonstrate the target concepts
- [ ] The capstone test exercises all 6 failure modes
- [ ] `scripts/progress.sh` correctly reports 0% when no exercises are done and 100% when all solutions are in place
- [ ] Every test file has 3‚Äì5 graded test functions ordered from simplest to hardest
- [ ] Every `assert` includes a diagnostic message naming the relevant failure mode
- [ ] Tests for exercise N never import from the learner's earlier exercise files (only from `src/tool_correctness/`)
- [ ] `docs/01_learning_path.md` has recall questions before every milestone (starting from Milestone 2)
- [ ] Read-exercise tests verify a concrete artifact (dict, list, or function return), not just mental answers
- [ ] `exercises/README.md` includes a troubleshooting FAQ with common mistakes per exercise category

---

**Output the complete repository as a set of files. Include every file listed in the structure above, fully populated. Do not use placeholder comments like "// rest of code here" ‚Äî every file must be complete and functional.**

**The repository should be saved to `tool-use-correctness/`.**
